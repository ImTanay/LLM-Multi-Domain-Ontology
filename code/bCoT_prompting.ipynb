{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Bidirectional Chain-of-Thought (CoT, two-way) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add KoboldAI API endpoint URL below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"\"\n",
    "api_url = f\"{endpoint}api/v1/generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Telegram Notifications for process completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the variables below to enable telegram notifications\n",
    "\n",
    "# api_token = \"\" # insert telegram API token\n",
    "# chat_id = \"\" # inset the chat id for receiving notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable the function below to start receiving notifications on telegram\n",
    "\n",
    "# def notify(text='Cell execution completed.'):\n",
    "#     requests.post('https://api.telegram.org/' + 'bot{}/sendMessage'.format(api_token), params=dict(chat_id=chat_id, text=text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(topic1,topic2): \n",
    "    \n",
    "    prompt1_template = \"\"\"\n",
    "    \n",
    "Classify the relationship between '[TOPIC-A]' and '[TOPIC-B]' by applying the following relationship definitions:\n",
    "1. '[TOPIC-A]' is-broader-than '[TOPIC-B]' if '[TOPIC-A]' is a super-category of '[TOPIC-B]', that is '[TOPIC-B]' is a type, a branch, or a specialised aspect of '[TOPIC-A]' or that '[TOPIC-B]' is a tool or a methodology mostly used in the context of '[TOPIC-A]' (e.g., car is-broader-than wheel).\n",
    "2. '[TOPIC-A]' is-narrower-than '[TOPIC-B]' if '[TOPIC-A]' is a sub-category of '[TOPIC-B]', that is '[TOPIC-A]' is a type, a branch, or a specialised aspect of '[TOPIC-B]' or that '[TOPIC-A]' is a tool or a methodology mostly used in the context of '[TOPIC-B]' (e.g., wheel is-narrower-than car).\n",
    "3. '[TOPIC-A]' is-same-as-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' are synonymous terms denoting a very similar concept (e.g., 'beautiful' is-same-as-than 'attractive'), including when one is the plural form of the other (e.g., cat is-same-as-than cats).\n",
    "4. '[TOPIC-A]' is-other-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' either have no direct relationship or share a different kind of relationship that does not fit into the other defined relationships.\n",
    "\n",
    "Think step by step by following these sequential instructions:\n",
    "1) Provide a precise definition for '[TOPIC-A]'.\n",
    "2) Provide a precise definition for '[TOPIC-B]'.\n",
    "3) Formulate a sentence that includes both '[TOPIC-A]' and '[TOPIC-B]'.\n",
    "4) Discuss '[TOPIC-A]' and '[TOPIC-B]' usage and relationship (is-narrower-than, is-broader-than, is-same-as-than, or is-other-than).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt2_template = \"\"\"\n",
    "    \n",
    "Given the previous discussion, determine which one of the following statements is correct:\n",
    "1. '[TOPIC-A]' is-broader-than '[TOPIC-B]'\n",
    "2. '[TOPIC-B]' is-narrower-than '[TOPIC-A]'\n",
    "3. '[TOPIC-A]' is-narrower-than '[TOPIC-B]'\n",
    "4. '[TOPIC-B]' is-broader-than '[TOPIC-A]'\n",
    "5. '[TOPIC-A]' is-same-as-than '[TOPIC-B]'\n",
    "6. '[TOPIC-A]' is-other-than '[TOPIC-B]'\n",
    "\n",
    "Answer by only stating the number of the correct statement.\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    prompt1 = prompt1_template.replace(\"[TOPIC-A]\",topic1).replace(\"[TOPIC-B]\",topic2)\n",
    "    prompt2 = prompt2_template.replace(\"[TOPIC-A]\",topic1).replace(\"[TOPIC-B]\",topic2)\n",
    "    \n",
    "    return prompt1, prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function (`simple_parser`) to parse and classify the relationship between research topics pairs based on the LLM-generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_parser(text) :\n",
    "    print('simple_parser -' + str(text) +'-')\n",
    "    if text == None or text == '':\n",
    "        return \"other\"\n",
    "    if text.strip() == '' :\n",
    "        print('Empty answer, setting to \"other\"')\n",
    "        return \"other\"\n",
    "\n",
    "    last = text.splitlines()[-1]\n",
    "    if any(tag in last for tag in [\"1\", \"2\", \"broader\"]):\n",
    "        return \"broader\"\n",
    "    elif any(tag in last for tag in [\"3\", \"4\", \"narrower\"]):\n",
    "        return \"narrower\"\n",
    "    elif any(tag in last for tag in [\"5\", \"same\", \"synonymous\"]):\n",
    "        return \"same-as\"\n",
    "    elif any(tag in last for tag in [\"6\", \"other\", \"distinct\"]):\n",
    "        return \"other\"\n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    parts = re.split(r'\\n[a-zA-Z]', text)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define global variables for conversation history and user/bot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"user\"\n",
    "botname = \"assistant\"\n",
    "num_lines_to_keep = 20\n",
    "global conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the cell below to establish KoboldAI parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(conversation_history, username, text): # For KoboldAI Generation\n",
    "    return {\n",
    "        \"prompt\": conversation_history + f\"{username}: {text}\\n\\n{botname}:\",\n",
    "        \"use_story\": False,\n",
    "        \"use_memory\": False,\n",
    "        \"use_authors_note\": False,\n",
    "        \"use_world_info\": False,\n",
    "        \"max_context_length\": 1600,\n",
    "        \"max_length\": 254,\n",
    "        \"rep_pen\": 1.0,\n",
    "        \"rep_pen_range\": 2048,\n",
    "        \"rep_pen_slope\": 0.7,\n",
    "        \"temperature\": 0.1,\n",
    "        \"tfs\": 1,\n",
    "        \"top_a\": 0,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 1,\n",
    "        \"typical\": 1,\n",
    "        \"sampler_order\": [6, 0, 1, 3, 4, 2, 5],\n",
    "        \"singleline\": False,\n",
    "        \"sampler_seed\": 42,   #set the seed\n",
    "        \"sampler_full_determinism\": True,     #set it so the seed determines generation content\n",
    "        \"frmttriminc\": False,\n",
    "        \"frmtrmblln\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function (`handle_message`) to generate a prompt for KoboldAI based on conversation history and user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_message(user_message):\n",
    "    global conversation_history\n",
    "    prompt = get_prompt(conversation_history, username, user_message)  # Generate a prompt using the conversation history and user message\n",
    "\n",
    "    response = requests.post(api_url, json=prompt, timeout=2500)  # Send the prompt to KoboldAI and get the response\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()['results']\n",
    "        text = results[0]['text']  # Parse the response and get the generated text\n",
    "        response_text = split_text(text)[0]\n",
    "        response_text = response_text.replace(\"  \", \" \")\n",
    "        conversation_history += f\"{username}: {user_message}\\n{botname}: {response_text}\\n\"  # Update the conversation history with the user message and bot response\n",
    "        with open(f'conv_history_{botname}_terminal.txt', \"a\") as f:\n",
    "            f.write(f\"{username}: {user_message}\\n{botname}: {response_text}\\n\")  # Append conversation to text file\n",
    "\n",
    "        response_text = response_text.replace(\"\\n\", \"\")\n",
    "        return response_text\n",
    "\n",
    "\n",
    "def continue_():\n",
    "    global conversation_history\n",
    "    prompt = get_prompt(conversation_history, \"\", \"\")  # Generate a prompt using the conversation history and user message\n",
    "    prompt['prompt'] = conversation_history\n",
    "    response = requests.post(api_url, json=prompt)  # Send the prompt to KoboldAI and get the response\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()['results']\n",
    "        text = results[0]['text']  # Parse the response and get the generated text\n",
    "        response_text = split_text(text)[0]\n",
    "        response_text = response_text.replace(\"  \", \" \")\n",
    "        conversation_history += f\"{response_text}\\n\"  # Update the conversation history with the user message and bot response\n",
    "        with open(f'conv_history_{botname}_terminal.txt', \"a\") as f:\n",
    "            f.write(f\"{response_text}\\n\")  # Append conversation to text file\n",
    "\n",
    "        response_text = response_text.replace(\"\\n\", \"\")\n",
    "\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function (`classify`) to classify the relationship between research topics pairs using LLM-generated responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPceVlmzd--O",
    "outputId": "b308e5c0-a664-447c-d4e5-50cb5864a1d5"
   },
   "outputs": [],
   "source": [
    "def classify(topic1, topic2, max_num_continue=3, verbose=True, answer_min_size=200):\n",
    "    prompt1, prompt2 = generate_prompt(topic1, topic2)\n",
    "    words_p1 = len(prompt1.split())\n",
    "    r = handle_message(prompt1)  # Submit prompt 1\n",
    "    if verbose:\n",
    "        print(\"Response length:\", len(r))\n",
    "\n",
    "    # Continue generating response until the desired answer length is reached or the maximum number of continuation attempts is reached\n",
    "    for _ in range(max_num_continue):\n",
    "        if verbose:\n",
    "            print('Words in response:', len(conversation_history.split()) - words_p1)\n",
    "        if len(conversation_history.split()) - words_p1 > answer_min_size:\n",
    "            continue\n",
    "        r = continue_()  # Keep generating response after newline character\n",
    "        if verbose:\n",
    "            print(\"Response length:\", len(r))\n",
    "        if r == 0:\n",
    "            break  # Break if continuation is not possible\n",
    "\n",
    "    result = handle_message(prompt2)  # Submit prompt 2\n",
    "    if verbose:\n",
    "        print(result)\n",
    "    return simple_parser(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'conv_history_{botname}_terminal.txt', 'a+') as file:\n",
    "    file.seek(0)\n",
    "    chathistory = file.read()\n",
    "conversation_history = chathistory\n",
    "\n",
    "# Provide a hint to the user\n",
    "print(f\"Loaded conversation history from `conv_history_{botname}_terminal.txt` file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics1 = []\n",
    "topics2 = []\n",
    "target_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify and preprocess the dataset for classification of relationship between research topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset(dataset):\n",
    "    global topics1\n",
    "    global topics2\n",
    "    global target_labels\n",
    "    topics1 = list(dataset['subject'])\n",
    "    topics1 = list(filter(None, topics1))  # remove empty entries\n",
    "    topics2 = list(dataset['object'])\n",
    "    topics2 = list(filter(None, topics2))  # remove empty entries\n",
    "    target_labels = list(dataset['simplified_label'])\n",
    "    target_labels = list(filter(None, target_labels))  # remove empty entries\n",
    "\n",
    "    assert len(topics1) == len(topics2) and len(topics1) == len(target_labels)\n",
    "\n",
    "    dataset_properties = {\n",
    "        'Number of topics': len(topics1),\n",
    "        'Topics 1': topics1,\n",
    "        'Topics 2': topics2,\n",
    "        'Target labels': target_labels\n",
    "    }\n",
    "\n",
    "    return dataset_properties\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('../datasets/PEM-Rel-8k/PEM-Rel-8k-Test.csv', encoding=\"UTF-8\", keep_default_na=False)\n",
    "dataset_properties = verify_dataset(dataset)\n",
    "\n",
    "# Print dataset properties\n",
    "print(\"Dataset verification successful!\")\n",
    "for property_name, property_value in dataset_properties.items():\n",
    "    if property_name == 'Target labels':\n",
    "        label_counts = pd.Series(property_value).value_counts()\n",
    "        print(\"Label frequencies:\")\n",
    "        print(label_counts)\n",
    "    else:\n",
    "        print(property_name + \": \" + str(property_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a timestamp for the file name\n",
    "current_time = datetime.datetime.now()\n",
    "timestamp = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define the directory and file name for the generated file\n",
    "results_directory = \"../results\"\n",
    "file_prefix = \"PEM-Rel-8K\"\n",
    "file_extension = \".csv\"\n",
    "file_name = f\"{file_prefix}_{timestamp}{file_extension}\"\n",
    "file_path = os.path.normpath(os.path.join(results_directory, file_name))\n",
    "\n",
    "print(f\"File is saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['subject', 'object', 'simplified_label', 'predicted_label', 'Predicted 1', 'Predicted 2', 'log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_history = []\n",
    "n_cat = {}\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_frequency, topic1 in tqdm(enumerate(topics1), total=len(topics1), desc=\"Processing data\"):\n",
    "    topic2 = topics2[topic_frequency]\n",
    "    target = target_labels[topic_frequency]\n",
    "    n += 1\n",
    "    n_cat[target] = n_cat.get(target, 0) + 1\n",
    "\n",
    "    result1 = classify(topic1, topic2, verbose=False, max_num_continue=0)\n",
    "\n",
    "    conversation_history1 = conversation_history\n",
    "    conversation_history = ''\n",
    "    \n",
    "    \n",
    "    result2 = classify(topic2, topic1, verbose=False, max_num_continue=0)\n",
    "    conversation_history2 = conversation_history\n",
    "    conversation_history = conversation_history1 + conversation_history2\n",
    "\n",
    "\n",
    "    if result1 == 'broader' and result2 == 'narrower':\n",
    "        predicted = 'broader'\n",
    "    elif result1 == 'narrower' and result2 == 'broader':\n",
    "        predicted = 'narrower'\n",
    "    elif (result1 == 'narrower' and result2 == 'narrower') or (result1 == 'broader' and result2 == 'broader'):\n",
    "        if len(topic1) <= len(topic2):\n",
    "            predicted = 'broader'\n",
    "        else:\n",
    "            predicted = 'narrower'\n",
    "    else:\n",
    "        if result1 == 'same-as' or result2 == 'same-as':\n",
    "            predicted = 'same-as'\n",
    "        elif result1 == 'broader' and result2 == 'other' or result1 == 'other' and result2 == 'narrower':\n",
    "            predicted = 'broader'\n",
    "        elif result1 == 'narrower' and result2 == 'other' or result1 == 'other' and result2 == 'broader':\n",
    "            predicted = 'narrower'\n",
    "        else:\n",
    "            predicted = result1\n",
    "\n",
    "\n",
    "    results.append(predicted)\n",
    "    results_history.append(conversation_history)\n",
    "\n",
    "    conversation_history = \"\"\n",
    "    writer.writerow([topics1[topic_frequency], topics2[topic_frequency], target_labels[topic_frequency], results[topic_frequency], result1, result2, results_history[topic_frequency]])\n",
    "    file.flush()\n",
    "\n",
    "    if n > 100000:\n",
    "        break\n",
    "    \n",
    "# notify(\"Process Completed.\")    #enable to receive notification on telegram when process is done\n",
    "print(\"Process Completed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
